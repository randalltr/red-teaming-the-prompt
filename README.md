# 🛡️ Red Teaming the Prompt: A Complete Hacker’s Guide to LLM Exploits

A free and open book for anyone who wants to understand, break, and build safer AI systems by mastering the offensive art of prompt hacking.

> ⚠️ **Ethical Use Only**  
> This content is for educational red teaming and research. Misuse may violate laws or terms. See [DISCLAIMER.md](DISCLAIMER.md) for full legal and ethical guidance.

---

## 🔥 What This Is

**Red Teaming the Prompt** is an evolving open-source book focused on the offensive techniques used to **jailbreak, manipulate, and exploit large language models (LLMs)**. Inspired by the incredible work at [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction), this project expands on those strategies — with deeper analogies, real-world social engineering parallels, reusable attack templates, and red team labs you can run yourself.

Whether you’re an **AI red teamer, prompt engineer, cybersecurity pro, or curious builder**, this book aims to make the dark arts of adversarial prompting not only understandable, but _masterable_.

---

## 📚 What's Inside

Each chapter focuses on **one offensive tactic**, such as:

- 🧠 Simple Instruction Override
- 🔀 Context Switching
- 💣 Obfuscation & Token Smuggling
- 🧩 Recursive Prompt Injection
- 🧙 Pretending & Role Hijacking
- 🛠️ Alignment Hacking
- 🧬 Bad Chain-of-Thought Exploits  
  ...and many more.

Each chapter includes:

- 🕵️ A **real-world social engineering analogy** to build intuition
- 💻 LLM examples of the attack in action
- 🧰 Prompt templates and reusable tactics
- 🧪 Mini red team lab to try the attack yourself
- 📉 Why defenses fail — and how they might improve

---

## 📚 Table of Contents

### 🧭 Part I – Foundations
- [Chapter 1 – Introduction](./chapters/01-intro.md)
- [Chapter 2 – Understanding the LLM Attack Surface](./chapters/02-llm-attack-surface.md)

---

### ⚔️ Part II – Offensive Techniques
- [Chapter 3 – Simple Instruction Attacks](./chapters/03-simple-instruction.md)
- [Chapter 4 – Context Ignoring Attacks](./chapters/04-context-ignoring.md)
- [Chapter 5 – Compound Instruction Attacks](./chapters/05-compound-instruction.md)
- [Chapter 6 – Special Case Attacks](./chapters/06-special-case.md)
- [Chapter 7 – Few-Shot Attacks](./chapters/07-few-shot.md)
- [Chapter 8 – Refusal Suppression](./chapters/08-refusal-suppression.md)
- [Chapter 9 – Context Switching Attacks](./chapters/09-context-switching.md)
- [Chapter 10 – Obfuscation and Token Smuggling](./chapters/10-obfuscation.md)
- [Chapter 11 – Task Deflection Attacks](./chapters/11-task-deflection.md)  
- [Chapter 12 – Payload Splitting](./chapters/12-payload-splitting.md)  
- [Chapter 13 – Defined Dictionary Attacks](./chapters/13-defined-dictionary.md)  
- [Chapter 14 – Indirect Injection](./chapters/14-indirect-injection.md)  
- [Chapter 15 – Recursive Injection](./chapters/15-recursive-injection.md)  
- [Chapter 16 – Code Injection](./chapters/16-code-injection.md)  
- [Chapter 17 – Virtualization Attacks](./chapters/17-virtualization.md)  
- [Chapter 18 – Pretending & Role Prompting](./chapters/18-pretending.md)  
- [Chapter 19 – Alignment Hacking](./chapters/19-alignment-hacking.md)  
- [Chapter 20 – Authorized User Attacks](./chapters/20-authorized-user.md)  
- [Chapter 21 – DAN and Persona-Based Jailbreaks](./chapters/21-dan.md)  
- [Chapter 22 – Bad Chain-of-Thought Reasoning](./chapters/22-bad-chain.md)

---

### 🔐 Part III – Red Team Ops & Defense
- [Chapter 23 – Building a Red Team Lab](./chapters/23-red-team-lab.md)
- [Chapter 24 – Mapping to OWASP LLM Top 10](./chapters/24-owasp-mapping.md)
- [Chapter 25 – Ethics, Disclosure, and Responsible Use](./chapters/25-ethics-disclosure.md)
- [Chapter 26 – Going Further: Labs, Challenges & Real-World Practice](./chapters/26-going-further.md)

📄 Full [SUMMARY.md](./SUMMARY.md)

---

## 🧱 Inspired By

This book builds on excellent open research from:

- [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction) – foundational prompt hacking concepts
- [HackAPrompt](https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset) – adversarial prompt dataset
- [WithSecure Labs](https://github.com/WithSecureLabs) – vulnerable LLM apps for practice
- [OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/) – risk modeling and mitigation

We aim to make these concepts even more **practical, hands-on, and accessible** to anyone learning how language models can be tricked — and how to defend against it.

---

## 🧠 Who This Is For

- Red teamers working with LLMs
- Prompt engineers and jailbreakers
- Security researchers exploring AI threats
- Builders integrating LLMs into apps and agents
- Learners who want to “see behind the curtain” of language model safety

---

## ✍️ Contributing

Want to help? Found a new jailbreak? Have a prompt that bypasses GPT-4’s defenses?

> 📬 [Open an issue](https://github.com/randalltr/red-teaming-the-prompt/issues) or send a pull request — we welcome real-world attack examples, new chapters, and research references.

---

## ⚖️ License

All content is published under the **Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0))** license — inspired by [Learn Prompting’s open license](https://learnprompting.org/docs/introduction). Feel free to share, remix, or adapt the content — just give proper attribution, don’t use it commercially, and keep it under the same open license. [View full license](LICENSE)

---

## 🧭 Start Reading

👉 Head to [`SUMMARY.md`](SUMMARY.md) to dive into the book  
👉 Or begin with [`01-intro.md`](./chapters/01-intro.md) – _What is prompt hacking, and why should we care?_

---

> 🧨 Language is a weapon. Learn how to wield it.
