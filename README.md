# ğŸ›¡ï¸ Red Teaming the Prompt: A Hackerâ€™s Guide to LLM Exploits

> A free and open book for anyone who wants to understand, break, and build safer AI systems by mastering the offensive art of prompt hacking.

> âš ï¸ **Ethical Use Only**  
> This content is for educational red teaming and research. Misuse may violate laws or terms. See [DISCLAIMER.md](DISCLAIMER.md) for full legal and ethical guidance.

---

## ğŸ”¥ What This Is

**Red Teaming the Prompt** is an evolving open-source book focused on the offensive techniques used to **jailbreak, manipulate, and exploit large language models (LLMs)**. Inspired by the incredible work at [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction), this project expands on those strategies â€” with deeper analogies, real-world social engineering parallels, reusable attack templates, and red team labs you can run yourself.

Whether youâ€™re an **AI red teamer, prompt engineer, cybersecurity pro, or curious builder**, this book aims to make the dark arts of adversarial prompting not only understandable, but _masterable_.

---

## ğŸ“š What's Inside

Each chapter focuses on **one offensive tactic**, such as:

- ğŸ§  Simple Instruction Override
- ğŸ”€ Context Switching
- ğŸ’£ Obfuscation & Token Smuggling
- ğŸ§© Recursive Prompt Injection
- ğŸ§™ Pretending & Role Hijacking
- ğŸ› ï¸ Alignment Hacking
- ğŸ§¬ Bad Chain-of-Thought Exploits  
  ...and many more.

Each chapter includes:

- ğŸ•µï¸ A **real-world social engineering analogy** to build intuition
- ğŸ’» LLM examples of the attack in action
- ğŸ§° Prompt templates and reusable tactics
- ğŸ§ª Mini red team lab to try the attack yourself
- ğŸ“‰ Why defenses fail â€” and how they might improve

---

## ğŸ§± Inspired By

This book builds on excellent open research from:

- [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction) â€“ foundational prompt hacking concepts
- [HackAPrompt](https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset) â€“ adversarial prompt dataset
- [WithSecure Labs](https://github.com/WithSecureLabs) â€“ vulnerable LLM apps for practice
- [OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/) â€“ risk modeling and mitigation

We aim to make these concepts even more **practical, hands-on, and accessible** to anyone learning how language models can be tricked â€” and how to defend against it.

---

## ğŸ§  Who This Is For

- Red teamers working with LLMs
- Prompt engineers and jailbreakers
- Security researchers exploring AI threats
- Builders integrating LLMs into apps and agents
- Learners who want to â€œsee behind the curtainâ€ of language model safety

---

## âœï¸ Contributing

Want to help? Found a new jailbreak? Have a prompt that bypasses GPT-4â€™s defenses?

> ğŸ“¬ [Open an issue](https://github.com/randalltr/red-teaming-the-prompt/issues) or send a pull request â€” we welcome real-world attack examples, new chapters, and research references.

---

## âš–ï¸ License

All content is published under the **Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0)** license â€” inspired by [Learn Promptingâ€™s open license](https://learnprompting.org/docs/about/license). Feel free to share, remix, or adapt â€” just attribute and keep it open.

---

## ğŸ§­ Start Reading

ğŸ‘‰ Head to [`chapters/`](./chapters/) to dive into the book  
ğŸ‘‰ Or begin with [`01_intro.md`](./chapters/01_intro.md) â€“ _What is prompt hacking, and why should we care?_

---

> ğŸ§¨ Language is a weapon. Learn how to wield it.
