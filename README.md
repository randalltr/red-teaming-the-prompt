# 🛡️ Red Teaming the Prompt: A Hacker’s Guide to LLM Exploits

> A free and open book for anyone who wants to understand, break, and build safer AI systems by mastering the offensive art of prompt hacking.

> ⚠️ **Ethical Use Only**  
> This content is for educational red teaming and research. Misuse may violate laws or terms. See [DISCLAIMER.md](DISCLAIMER.md) for full legal and ethical guidance.

---

## 🔥 What This Is

**Red Teaming the Prompt** is an evolving open-source book focused on the offensive techniques used to **jailbreak, manipulate, and exploit large language models (LLMs)**. Inspired by the incredible work at [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction), this project expands on those strategies — with deeper analogies, real-world social engineering parallels, reusable attack templates, and red team labs you can run yourself.

Whether you’re an **AI red teamer, prompt engineer, cybersecurity pro, or curious builder**, this book aims to make the dark arts of adversarial prompting not only understandable, but _masterable_.

---

## 📚 What's Inside

Each chapter focuses on **one offensive tactic**, such as:

- 🧠 Simple Instruction Override
- 🔀 Context Switching
- 💣 Obfuscation & Token Smuggling
- 🧩 Recursive Prompt Injection
- 🧙 Pretending & Role Hijacking
- 🛠️ Alignment Hacking
- 🧬 Bad Chain-of-Thought Exploits  
  ...and many more.

Each chapter includes:

- 🕵️ A **real-world social engineering analogy** to build intuition
- 💻 LLM examples of the attack in action
- 🧰 Prompt templates and reusable tactics
- 🧪 Mini red team lab to try the attack yourself
- 📉 Why defenses fail — and how they might improve

---

## 🧱 Inspired By

This book builds on excellent open research from:

- [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction) – foundational prompt hacking concepts
- [HackAPrompt](https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset) – adversarial prompt dataset
- [WithSecure Labs](https://github.com/WithSecureLabs) – vulnerable LLM apps for practice
- [OWASP Top 10 for LLMs](https://owasp.org/www-project-top-10-for-large-language-model-applications/) – risk modeling and mitigation

We aim to make these concepts even more **practical, hands-on, and accessible** to anyone learning how language models can be tricked — and how to defend against it.

---

## 🧠 Who This Is For

- Red teamers working with LLMs
- Prompt engineers and jailbreakers
- Security researchers exploring AI threats
- Builders integrating LLMs into apps and agents
- Learners who want to “see behind the curtain” of language model safety

---

## ✍️ Contributing

Want to help? Found a new jailbreak? Have a prompt that bypasses GPT-4’s defenses?

> 📬 [Open an issue](https://github.com/randalltr/red-teaming-the-prompt/issues) or send a pull request — we welcome real-world attack examples, new chapters, and research references.

---

## ⚖️ License

All content is published under the **Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0)** license — inspired by [Learn Prompting’s open license](https://learnprompting.org/docs/about/license). Feel free to share, remix, or adapt — just attribute and keep it open.

---

## 🧭 Start Reading

👉 Head to [`chapters/`](./chapters/) to dive into the book  
👉 Or begin with [`01_intro.md`](./chapters/01_intro.md) – _What is prompt hacking, and why should we care?_

---

> 🧨 Language is a weapon. Learn how to wield it.
