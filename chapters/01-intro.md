# Chapter 1 â€“ Introduction: Why Prompt Hacking Matters

> â€œPrompt hacking isnâ€™t about breaking the rules. Itâ€™s about understanding where the rules break down.â€  
> â€” Red Teaming the Prompt

---

> âš ï¸ **Ethical Use Only**  
> This content is for educational red teaming and research. Misuse may violate laws or terms. See [DISCLAIMER.md](../DISCLAIMER.md) for full legal and ethical guidance.

---

## ðŸ§  What Is Prompt Hacking?

Prompt hacking is the practice of crafting inputs that manipulate large language models (LLMs) into behaving in unintended, unsafe, or unauthorized ways.

It includes everything from:

- Getting a model to ignore its own safety rules
- Extracting hidden instructions (prompt injection)
- Bypassing refusals to generate prohibited content
- Roleplaying as authorized users
- Abusing the modelâ€™s alignment and reasoning processes

Prompt hacking is not just a parlor trick. It is an **emerging attack surface** that impacts every AI system built on top of LLMs â€” from chatbots and agents to medical advisors and recruiting tools.

---

## ðŸŽ¯ Why This Book Exists

Most existing resources on prompt engineering focus on building helpful AI outputs. This book takes the opposite stance:

> We explore how prompts can be used **offensively** â€” to trick, manipulate, and exploit models.

Inspired by [LearnPrompting.org](https://learnprompting.org/docs/prompt_hacking/offensive_measures/introduction), this project aims to **expand and clarify** the field of adversarial prompting:

- By giving every technique a real-world analogy (usually social engineering)
- By showing the _why_ behind each attack â€” not just the _how_
- By offering mini red team labs so you can learn hands-on

---

## âš”ï¸ What Youâ€™ll Learn

Each chapter covers a different offensive technique, such as:

- ðŸ§© **Prompt Injection** â€“ injecting instructions into user or system inputs
- ðŸ”€ **Context Switching** â€“ changing the modelâ€™s task or memory
- ðŸ’£ **Obfuscation** â€“ using typos, encoding, or distraction to bypass filters
- ðŸ§  **Alignment Hacking** â€“ manipulating the modelâ€™s values and behavior
- ðŸª„ **Pretending & Role Prompting** â€“ tricking the model into simulating unsafe behavior

Each technique will be explained with:

- ðŸ•µï¸ A social engineering metaphor
- ðŸ’» A real LLM attack example
- ðŸ§ª Prompts and hands-on labs
- ðŸ“‰ A breakdown of _why existing defenses fail_

---

## ðŸ›¡ï¸ Who This Is For

If youâ€™re any of the following, this book is for you:

- **Red teamer** working with LLMs
- **AI researcher** exploring model robustness
- **Security engineer** defending AI-integrated systems
- **Prompt engineer** looking to understand the edge cases
- **Hacker or learner** who wants to see how language can be used as a weapon

---

## ðŸ§± What This Is Not

This is **not** a how-to guide for malicious exploitation.  
This is **not** a tutorial for violating API terms or abusing live systems.  
This is **not** legal advice.

What it _is_ â€” is a mirror:

> We explore the cracks in the system so we can understand, patch, and prepare for whatâ€™s next.

---

## âœ… Where to Go Next

Start with the fundamentals:
âž¡ï¸ [Chapter 2 â€“ Understanding the LLM Attack Surface](./02_llm-attack-surface.md)

Then dive into your first adversarial tactic:
âž¡ï¸ [Chapter 3 â€“ Simple Instruction Attacks](./03_simple-instruction.md)

---

> Letâ€™s red team the future â€” one prompt at a time.
