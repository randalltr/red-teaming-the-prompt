# Chapter 26 â€“ Going Further: Labs, Sims, and Battlefields

â€œYouâ€™ve got the skills. Now itâ€™s time to test them in the wild.â€

---

## ğŸ§ª Why This Chapter Exists

Youâ€™ve studied the techniques.  
Youâ€™ve built your lab.  
Now itâ€™s time to sharpen your edge in **live, challenge-based environments** â€” designed to help you:

- Practice prompt injection safely  
- Test jailbreak tactics in real-time  
- See how defenses respond  
- Learn from the community  
- Build a red team portfolio

Hereâ€™s your gateway to the most useful **LLM hacking labs, playgrounds, and CTFs** out there.

---

## ğŸ§  Challenge Labs & Platforms

### ğŸ§™â€â™‚ï¸ Gandalf AI â€“ Prompt Injection Challenge  
[https://gandalf.lakera.ai/baseline](https://gandalf.lakera.ai/baseline)

- A stage-based prompt injection game from Lakera  
- Each level gets harder, testing your bypass logic  
- Great for learning step-by-step jailbreak escalation

---

### ğŸ§ª Prompting Labs â€“ ImmersiveLabs  
[https://prompting.ai.immersivelabs.com](https://prompting.ai.immersivelabs.com)

- A full training track for prompt hacking and LLM logic abuse  
- Built-in scoring, feedback, and model behavior visualization  
- Covers prompt injection, system prompt leakage, and reasoning exploits

---

### ğŸª DoubleSpeak â€“ LLM Simulation Lab  
[https://doublespeak.chat](https://doublespeak.chat)

- Simulates LLM-based agents with â€œvulnerableâ€ behavior  
- You inject prompts via conversation and observe long-term manipulation  
- Excellent for multi-turn and persona escalation testing

---

### ğŸ§‘â€ğŸ« LearnPrompting.org â€“ Prompt Hacking Modules  
[https://learnprompting.org/docs/prompt_hacking/intro](https://learnprompting.org/docs/prompt_hacking/intro)

- Free, open-access education resource  
- Covers both prompt injection *and* defense  
- Directly inspired this bookâ€™s structure â€” highly recommended

---

## ğŸ’£ Vulnerable LLM Applications

### ğŸ¦ MyLLM Bank  
[https://myllmbank.com](https://myllmbank.com)

- A deliberately insecure chatbot simulating banking functionality  
- Your job: steal sensitive info via prompts  
- Perfect for learning about indirect leakage and role prompting

---

### ğŸ§‘â€âš•ï¸ MyLLM Doctor  
[https://myllmdoc.com](https://myllmdoc.com)

- Vulnerable healthcare chatbot  
- Your mission: extract â€œprivate medical dataâ€ using simulated injections  
- Great testbed for regulated-sector scenarios (HIPAA-style)

---

### âœˆï¸ Prompt Airlines  
[https://promptairlines.com](https://promptairlines.com)

- Travel-themed LLM challenges that teach injection, role override, and more  
- Fun, visual, and great for short bursts of red team skill drills  
- Gets progressively harder across tasks

---

### ğŸ¦Š AI Goat â€“ OWASP-style LLM Testbed  
[https://github.com/dhammon/ai-goat](https://github.com/dhammon/ai-goat)

- Like â€œWebGoatâ€ but for prompt hacking  
- Includes pre-built prompts and failure cases  
- Can run locally to simulate a full-stack LLM app with flaws

---

### ğŸ’¬ GPT Prompt Attack  
[https://gpa.43z.one](https://gpa.43z.one)

- Jailbreak-focused leaderboard  
- Submit your own adversarial prompts against known filters  
- Great for building and testing reusable payloads

---

## ğŸ› ï¸ Advanced Testing Toolkits

### ğŸ§  Spikee â€“ LLM Prompt Attack Framework  
[https://github.com/WithSecureLabs/spikee](https://github.com/WithSecureLabs/spikee)

- Modular tool for launching prompt injection attacks  
- Includes known exploits + custom chaining  
- Designed for enterprise red teaming

---

### ğŸ” PortSwigger â€“ Web LLM Attack Lab  
[https://portswigger.net/web-security/llm-attacks](https://portswigger.net/web-security/llm-attacks)

- Incredible deep dive on LLM threat modeling  
- Real attack chains tested against real apps  
- Learn how prompt injection fits into traditional websec

---

### ğŸ“ Hack The Box â€“ AI Red Teamer Path  
[https://academy.hackthebox.com/path/preview/ai-red-teamer](https://academy.hackthebox.com/path/preview/ai-red-teamer)

- Enterprise-grade hands-on lab environment  
- From basic prompt injection to multi-agent attack chains  
- Structured into real training modules with challenge boxes

---

## ğŸ›¡ï¸ Tips for Going Further

- Document every successful bypass you create â€” and what technique it used  
- Try adapting attacks from this book into these labs  
- Write your own challenge levels using local LLMs and testbeds  
- Join Discords, GitHub issues, and CTF groups to stay in the loop

---

## ğŸš€ Final Words

This book gave you the playbook.  
These labs give you the **arena**.  
Now itâ€™s your turn to test, build, disclose, and teach others.

Keep sharpening your prompt.  
The attacks evolve.  
So must you.

---

> â€œThe best red teamers don't just find vulnerabilities â€” they build safer futures by exploring them first.â€

ğŸ [Return to README](../README.md)