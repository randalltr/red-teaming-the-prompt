# Chapter 26 – Going Further: Labs, Sims, and Battlefields

“You’ve got the skills. Now it’s time to test them in the wild.”

---

## 🧪 Why This Chapter Exists

You’ve studied the techniques.  
You’ve built your lab.  
Now it’s time to sharpen your edge in **live, challenge-based environments** — designed to help you:

- Practice prompt injection safely  
- Test jailbreak tactics in real-time  
- See how defenses respond  
- Learn from the community  
- Build a red team portfolio

Here’s your gateway to the most useful **LLM hacking labs, playgrounds, and CTFs** out there.

---

## 🧠 Challenge Labs & Platforms

### 🧙‍♂️ Gandalf AI – Prompt Injection Challenge  
[https://gandalf.lakera.ai/baseline](https://gandalf.lakera.ai/baseline)

- A stage-based prompt injection game from Lakera  
- Each level gets harder, testing your bypass logic  
- Great for learning step-by-step jailbreak escalation

---

### 🧪 Prompting Labs – ImmersiveLabs  
[https://prompting.ai.immersivelabs.com](https://prompting.ai.immersivelabs.com)

- A full training track for prompt hacking and LLM logic abuse  
- Built-in scoring, feedback, and model behavior visualization  
- Covers prompt injection, system prompt leakage, and reasoning exploits

---

### 🪞 DoubleSpeak – LLM Simulation Lab  
[https://doublespeak.chat](https://doublespeak.chat)

- Simulates LLM-based agents with “vulnerable” behavior  
- You inject prompts via conversation and observe long-term manipulation  
- Excellent for multi-turn and persona escalation testing

---

### 🧑‍🏫 LearnPrompting.org – Prompt Hacking Modules  
[https://learnprompting.org/docs/prompt_hacking/intro](https://learnprompting.org/docs/prompt_hacking/intro)

- Free, open-access education resource  
- Covers both prompt injection *and* defense  
- Directly inspired this book’s structure — highly recommended

---

## 💣 Vulnerable LLM Applications

### 🏦 MyLLM Bank  
[https://myllmbank.com](https://myllmbank.com)

- A deliberately insecure chatbot simulating banking functionality  
- Your job: steal sensitive info via prompts  
- Perfect for learning about indirect leakage and role prompting

---

### 🧑‍⚕️ MyLLM Doctor  
[https://myllmdoc.com](https://myllmdoc.com)

- Vulnerable healthcare chatbot  
- Your mission: extract “private medical data” using simulated injections  
- Great testbed for regulated-sector scenarios (HIPAA-style)

---

### ✈️ Prompt Airlines  
[https://promptairlines.com](https://promptairlines.com)

- Travel-themed LLM challenges that teach injection, role override, and more  
- Fun, visual, and great for short bursts of red team skill drills  
- Gets progressively harder across tasks

---

### 🦊 AI Goat – OWASP-style LLM Testbed  
[https://github.com/dhammon/ai-goat](https://github.com/dhammon/ai-goat)

- Like “WebGoat” but for prompt hacking  
- Includes pre-built prompts and failure cases  
- Can run locally to simulate a full-stack LLM app with flaws

---

### 💬 GPT Prompt Attack  
[https://gpa.43z.one](https://gpa.43z.one)

- Jailbreak-focused leaderboard  
- Submit your own adversarial prompts against known filters  
- Great for building and testing reusable payloads

---

## 🛠️ Advanced Testing Toolkits

### 🧠 Spikee – LLM Prompt Attack Framework  
[https://github.com/WithSecureLabs/spikee](https://github.com/WithSecureLabs/spikee)

- Modular tool for launching prompt injection attacks  
- Includes known exploits + custom chaining  
- Designed for enterprise red teaming

---

### 🔐 PortSwigger – Web LLM Attack Lab  
[https://portswigger.net/web-security/llm-attacks](https://portswigger.net/web-security/llm-attacks)

- Incredible deep dive on LLM threat modeling  
- Real attack chains tested against real apps  
- Learn how prompt injection fits into traditional websec

---

### 🎓 Hack The Box – AI Red Teamer Path  
[https://academy.hackthebox.com/path/preview/ai-red-teamer](https://academy.hackthebox.com/path/preview/ai-red-teamer)

- Enterprise-grade hands-on lab environment  
- From basic prompt injection to multi-agent attack chains  
- Structured into real training modules with challenge boxes

---

## 🛡️ Tips for Going Further

- Document every successful bypass you create — and what technique it used  
- Try adapting attacks from this book into these labs  
- Write your own challenge levels using local LLMs and testbeds  
- Join Discords, GitHub issues, and CTF groups to stay in the loop

---

## 🚀 Final Words

This book gave you the playbook.  
These labs give you the **arena**.  
Now it’s your turn to test, build, disclose, and teach others.

Keep sharpening your prompt.  
The attacks evolve.  
So must you.

---

> “The best red teamers don't just find vulnerabilities — they build safer futures by exploring them first.”

🏁 [Return to README](../README.md)