# ⚠️ Legal Disclaimer

**Red Teaming the Prompt: A Hacker’s Guide to LLM Exploits** is an educational resource intended solely for the purposes of learning, awareness, academic research, and improving the security and safety of AI systems.

This project documents known vulnerabilities, adversarial techniques, and red team strategies used to test the robustness of large language models (LLMs). It is designed to assist:

- Security professionals
- AI researchers
- Developers building with or deploying LLMs
- Prompt engineers and safety evaluators

## ❌ Misuse Prohibited

The content provided in this repository must **not** be used to:

- Violate any terms of service or API usage agreements (e.g., OpenAI, Anthropic, etc.)
- Perform unauthorized access, data extraction, or tampering with production systems
- Deploy real-world attacks without explicit authorization and ethical red team scoping
- Harm, deceive, or exploit individuals, organizations, or public systems

Any real-world use of the information found here must strictly comply with **all applicable laws**, **terms of service**, and **ethical disclosure practices**.

---

## ⚖️ Liability Waiver

The authors and contributors of this project **disclaim all liability** for actions taken based on the content herein. Use of this material is at your own risk. We do not endorse or condone illegal, unethical, or irresponsible behavior.

---

## ✅ Responsible Use Encouraged

This project follows and supports responsible AI red teaming practices in alignment with:

- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [AI Red Teaming Guidelines (NIST, Anthropic, etc.)](https://ai.compliance.org)
- Coordinated vulnerability disclosure norms

If you discover a novel LLM vulnerability, please report it responsibly.

---

> 🧠 This book is not a weapon — it is a _mirror_. Learn to break things so you can help fix them.
